\documentclass[letterpaper]{article}
\title{16831 Statistical Techniques, Fall 2011\\Homework 5: Online Learning}
\date{\textbf{Due:} Monday December 5th, emailed by the beginning of class}
\author{You should work in groups of 2 or 3.}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{url}
\usepackage{graphicx}
\usepackage{color}
% Make URL click-able
\usepackage{hyperref}

% Don't indent after paragraphs
\parindent 0in
\parskip 4pt

\begin{document}

\maketitle

\section*{Classifiers}

For this homework, we implemented Gaussian Process Regression (GPR), Boosting, and Online linear SVMs. GPR and boosting were not implemented in an online fashion (though boosting does lend itself to an online implementation, but we were limited in time on this homework). Gaussian Process Regression uses the covariance function, while boosting performs feature selection. We implemented Alex Grubb's version of boosting which uses the exponentiated gradient to produce our results. For the GPR, we used the exponential to the negative squared distance between features (radial basis function) as the covariance function.

\section*{Performance Summary}

For this homework, we performed two-fold cross-validation on data from one file, and then tested with data from the other file. We swapped the two files, and re-did cross-validation and training. We provide the best parameters, confusion matrices, per-class percentage performance, and net classification rate for the three classes. 

\subsection*{Gaussian Process Regression}

\textbf{Train with first file, test with the second file:}

Best parameters: radial basis function parameter $\sigma=.4$, regularization parameter $\lambda=.4642$.

Net classification rate: .8637

Per-class classification rate: 
$$\begin{bmatrix}0.8208   & 0.8870  &  0.7192   & 0.9796 &   0.6670\end{bmatrix}$$

Confusion matrix:
$$\begin{bmatrix}
0.8208 &   0.0560  & 0.0407 &   0.0015  &  0.0809\\
    0.0758&    0.8870 &   0.0084&    0.0004 &   0.0285\\
    0.1971   & 0.0092   & 0.7192  &  0.0046   & 0.0700\\
    0.0018   & 0.0179    &0.0003  &  0.9796  &  0.0005\\
    0.1316   &0.1576    &0.0421  &  0.0017  &  0.6670
\end{bmatrix}$$

\textbf{Train with first file, test with the second file:}

Best parameters: radial basis function parameter $\sigma=.4$, regularization parameter $\lambda=.4642$.

Net classification rate: .947

Per-class classification rate: 
$$\begin{bmatrix}0.8097  &  0.5697   & 0.9279   & 0.9899  &  0.8305\end{bmatrix}$$

Confusion matrix:
$$\begin{bmatrix}
0.8097&    0.0471&    0.0680  &  0.0010  &  0.0743\\
    0.0685&    0.5697&    0.0183&    0.0697  &  0.2738\\
    0.0378   & 0.0014&    0.9279  &       0 &   0.0329\\
    0.0020 &   0.0001 &   0.0065 &   0.9899  &  0.0014\\
    0.0611   & 0.0391  &  0.0686  &  0.0007&    0.8305\\
\end{bmatrix}$$


\subsection*{Linear SVMs}

\subsection*{AdaBoost}

\textbf{Train with first file, test with the second file:}

Best parameters: radial basis function parameter $\sigma=.4$, regularization parameter $\lambda=.4642$.

Net classification rate: .8637

Per-class classification rate: 
$$\begin{bmatrix}0.8208   & 0.8870  &  0.7192   & 0.9796 &   0.6670\end{bmatrix}$$

Confusion matrix:
$$\begin{bmatrix}
0.8208 &   0.0560  & 0.0407 &   0.0015  &  0.0809\\
    0.0758&    0.8870 &   0.0084&    0.0004 &   0.0285\\
    0.1971   & 0.0092   & 0.7192  &  0.0046   & 0.0700\\
    0.0018   & 0.0179    &0.0003  &  0.9796  &  0.0005\\
    0.1316   &0.1576    &0.0421  &  0.0017  &  0.6670
\end{bmatrix}$$

\textbf{Train with first file, test with the second file:}

Best parameters: radial basis function parameter $\sigma=.4$, regularization parameter $\lambda=.4642$.

Net classification rate: .947

Per-class classification rate: 
$$\begin{bmatrix}0.8097  &  0.5697   & 0.9279   & 0.9899  &  0.8305\end{bmatrix}$$

Confusion matrix:
$$\begin{bmatrix}
0.8097&    0.0471&    0.0680  &  0.0010  &  0.0743\\
    0.0685&    0.5697&    0.0183&    0.0697  &  0.2738\\
    0.0378   & 0.0014&    0.9279  &       0 &   0.0329\\
    0.0020 &   0.0001 &   0.0065 &   0.9899  &  0.0014\\
    0.0611   & 0.0391  &  0.0686  &  0.0007&    0.8305\\
\end{bmatrix}$$

We find that points belonging to the 'Pole' and 'Ground' classes do not perform well
because there is not enough data to train them.

GPR and SVM were relatively easy to implement. AdaBoost required a bit of delving into some math, but 
we made it through.



\begin{enumerate}
\item How well did it perform for online learning?  Does it perform well on the held-out data?
\item Are there any classes that did not get classified well?  Why do you think that is?
\item How easy was the learner to implement?
\item How long does the learner take (in terms of data points, dimensions, classes, etc...) for training and prediction?
\item Show images/movies of the classified data.  Note that MATLAB is not very good at displaying thousands of 3D points; use VRML or python.
\item How did you choose (hyper)parameters (priors, kernel width, noise variance, prior variance, learning rate, etc\ldots)?
\item How robust is this algorithm to noise? Take the current feature set and:
  \begin{itemize}
  \item Add a large number of random features
  \item Add a large number of features that are noise corrupted versions of the features already in the data-set.
  \end{itemize}

\end{enumerate}

You should also compare the learners' performance to each other.  Did kernels help on this data set?  Which one would you use on your robot?  What would future work include?

\end{document}
